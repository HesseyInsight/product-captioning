{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import generator\n",
    "import discriminator\n",
    "import helpers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwkim\\Desktop\\seqGAN-master_학습모델 포함\\generator.py:30: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(p, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "CUDA = True\n",
    "VOCAB_SIZE = 12028\n",
    "MAX_SEQ_LEN = 30\n",
    "START_LETTER = 0\n",
    "BATCH_SIZE = 64\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "ADV_TRAIN_EPOCHS = 20\n",
    "POS_NEG_SAMPLES = 10700 # 10847 \n",
    "\n",
    "GEN_EMBEDDING_DIM = 200\n",
    "GEN_HIDDEN_DIM = 64\n",
    "DIS_EMBEDDING_DIM = 64\n",
    "DIS_HIDDEN_DIM = 64\n",
    "idx = 0\n",
    "\n",
    "def decode(sentence, itov):\n",
    "    return \" \".join(itov[int(i)] for i in sentence)\n",
    "\n",
    "def getbatch(data, s, e):\n",
    "    temp = data.cpu()\n",
    "    if len(temp) < e - s:\n",
    "        return data\n",
    "    start, end = s % len(temp), e % len(temp)\n",
    "    if end < start:\n",
    "        res = np.vstack([temp[start:], temp[:end]])\n",
    "    else:\n",
    "        res = temp[start:end]\n",
    "    return torch.LongTensor(res)\n",
    "\n",
    "def train_generator_MLE(gen, gen_opt, oracle, real_data_samples, epochs):\n",
    "    \"\"\"\n",
    "    Max Likelihood Pretraining for the generator\n",
    "    \"\"\"\n",
    "    print(\"epoch \", epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch %d : ' % (epoch + 1), end='')\n",
    "        sys.stdout.flush()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "            inp, target = helpers.prepare_generator_batch(real_data_samples[i:i + BATCH_SIZE], start_letter=START_LETTER,\n",
    "                                                          gpu=CUDA)\n",
    "            gen_opt.zero_grad()\n",
    "            loss = gen.batchNLLLoss(inp, target)\n",
    "            loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (i / BATCH_SIZE) % ceil(\n",
    "                            ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # each loss in a batch is loss per sample\n",
    "        total_loss = total_loss / ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / MAX_SEQ_LEN\n",
    "\n",
    "        # sample from generator and compute oracle NLL\n",
    "        oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "        print(' average_train_NLL = %.4f, oracle_sample_NLL = %.4f' % (total_loss, oracle_loss))\n",
    "\n",
    "\n",
    "def train_generator_PG(gen, gen_opt, oracle, dis, num_batches):\n",
    "    \"\"\"\n",
    "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
    "    Training is done for num_batches batches.\n",
    "    \"\"\"\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        s = gen.sample(BATCH_SIZE*2)        # 64 works best\n",
    "        inp, target = helpers.prepare_generator_batch(s, start_letter=START_LETTER, gpu=CUDA)\n",
    "        rewards = dis.batchClassify(target)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        pg_loss = gen.batchPGLoss(inp, target, rewards)\n",
    "        pg_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "    # sample from generator and compute oracle NLL\n",
    "    oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "    print(' oracle_sample_NLL = %.4f' % oracle_loss)\n",
    "\n",
    "\n",
    "def train_discriminator(discriminator, dis_opt, real_data_samples, generator, oracle, d_steps, epochs):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # generating a small validation set before training (using oracle and generator)\n",
    "    pos_val = oracle.sample(100)\n",
    "    neg_val = generator.sample(100)\n",
    "    val_inp, val_target = helpers.prepare_discriminator_data(pos_val, neg_val, gpu=CUDA)\n",
    "\n",
    "    for d_step in range(d_steps):\n",
    "        s = helpers.batchwise_sample(generator, POS_NEG_SAMPLES, BATCH_SIZE)\n",
    "        dis_inp, dis_target = helpers.prepare_discriminator_data(real_data_samples, s, gpu=CUDA)\n",
    "        for epoch in range(epochs):\n",
    "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            for i in range(0, 2 * POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "                inp, target = dis_inp[i:i + BATCH_SIZE], dis_target[i:i + BATCH_SIZE]\n",
    "                dis_opt.zero_grad()\n",
    "                out = discriminator.batchClassify(inp)\n",
    "                loss_fn = nn.BCELoss()\n",
    "                loss = loss_fn(out, target)\n",
    "                loss.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "                total_loss += loss.data.item()\n",
    "                total_acc += torch.sum((out>0.5)==(target>0.5)).data.item()\n",
    "\n",
    "                if (i / BATCH_SIZE) % ceil(ceil(2 * POS_NEG_SAMPLES / float(\n",
    "                        BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                    print('.', end='')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            total_loss /= ceil(2 * POS_NEG_SAMPLES / float(BATCH_SIZE))\n",
    "            total_acc /= float(2 * POS_NEG_SAMPLES)\n",
    "\n",
    "            val_pred = discriminator.batchClassify(val_inp)\n",
    "            print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
    "                total_loss, total_acc, torch.sum((val_pred>0.5)==(val_target>0.5)).data.item()/200.))\n",
    "\n",
    "\n",
    "real_data = pickle.load(open('./mydata/real_data.txt', 'rb'))   # real_data\n",
    "emb_mat = pickle.load(open('./mydata/word2vec.txt', 'rb'))    # embedding matrix\n",
    "vocab_to_int = pickle.load(open('./mydata/word2idx.txt', 'rb')) # vocab to int\n",
    "int_to_vocab = pickle.load(open('./mydata/idx2word.txt', 'rb')) # int to vocab\n",
    "\n",
    "\n",
    "hidden_mat = torch.FloatTensor(np.random.rand(1, BATCH_SIZE, GEN_HIDDEN_DIM))\n",
    "real_data = torch.LongTensor(real_data)\n",
    "emb_mat = torch.tensor(emb_mat)\n",
    "inp = torch.LongTensor([real_data[i][idx] for i in range(BATCH_SIZE)])\n",
    "if CUDA:\n",
    "    hidden_mat = hidden_mat.cuda()\n",
    "    real_data = real_data.cuda()\n",
    "    emb_mat = emb_mat.cuda()\n",
    "    inp = inp.cuda()\n",
    "\n",
    "\n",
    "oracle = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA, oracle_init=True)\n",
    "gen = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "dis = discriminator.Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "oracle_samples = real_data\n",
    "if CUDA:\n",
    "    oracle = oracle.cuda()\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    oracle_samples = oracle_samples.cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dic = torch.load(\"seq30_b64_dim_200_v12028_mlep100_advp_20_posneg_10700.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gen', 'dis', 'oracle', 'i2v', 'v2i'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.load_state_dict(model_dic['gen'])\n",
    "dis.load_state_dict(model_dic['dis'])\n",
    "oracle.load_state_dict(model_dic['oracle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙증맞은 스커트 와 캐 주얼 하고 시크 함 과 함께 구성 된 면 이 더욱 참 변별 력 있는 폭 , 발볼 너비 를 생략 해 주시 면 위치 에\n",
      "드레스 업 아이템 이 가능하니까 , 가볍게 들기 좋은 가방 이에요 길이 로 일단 버핏 과 발목 마감 된 디자인 이라 더욱 시크 하게 힘 이 기대 되는 쇼츠 라서\n",
      "페니 로퍼입니 다 채 , 내부 부터 정이 고 캐 주얼 하 게 활용 하시면 좋을 것 같구요 기대 이상 으로 요 하이 웨 이스트 디자인 과 발목 까지 정직하게\n",
      "을 널찍하게 스타 일링 이 가능한 매력 적 인 발볼 이 매력 적 인 가방 이에요 다만 가방 이에요 따로 , 우아한 데 일리 로 , 서브 서브 서브 에코\n",
      "수 있는 디자인 의 그래서 보통 원단 퀄리티 를 입더라도 보면 , 이런 효과 가 있는 가방 이에요 다만 살짝 부분 크로스 로 스트랩 입구 를 조이는 이용 해 주심\n",
      "예요 캐 주얼 하고 , 비주 얼 이에요 수납 공간 과 더 감각 적 인 착 화 감도 예쁘셔요 캐 주얼 하 게 매치 해서 너무 피트 되지 않아서 오래\n",
      "너무 기본 로프 을 잡아 주는 역시 베이지 사이즈 라 특히 나 유용하실 거 예요 다만 , 발볼 과 자연 스럽게 착용 하기 좋아요 ♥ 보기 에도 입체 적 인\n",
      "넣어 계절 감 에도 자연 스럽게 소화 되는 팬츠 예요 쉽게 입문 템 백 이에요 장점 이에요 오히려 무조건 추천 해요 특유 , 숄 더크로스 가 예쁘게 담긴 소재 나\n",
      "가벼운 만큼 두고두고 소지품 수납 가능하셔요 무리 없이 실용 성 긴 팔 이나 등 이나 하이 웨 이스트 타입 이라 가벼운 멋 이 어울리는 베이지 , 화이트 세 컬러 로\n",
      "내 구도 로 잘 어울리거든요 캐 주얼 하 게 툭 주셔도 어렵지 않아서 오히려 데 일리 스타일 보다는 좀 아니구요 스타 일링 해 주시 면 참 예쁘답니다 다른 핏 경험\n"
     ]
    }
   ],
   "source": [
    "for i in gen.sample(10, start_letter=vocab_to_int[\"<start>\"]):\n",
    "    print(decode(i, int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
